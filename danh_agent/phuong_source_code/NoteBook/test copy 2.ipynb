{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = prompt_template = \\\n",
    "'''\n",
    "Function:\n",
    "def get_weather_data(coordinates):\n",
    "    \"\"\"\n",
    "    Fetches weather data from the Open-Meteo API for the given latitude and longitude.\n",
    "\n",
    "    Args:\n",
    "    coordinates (tuple): The latitude of the location.\n",
    "\n",
    "    Returns:\n",
    "    float: The current temperature in the coordinates you've asked for\n",
    "    \"\"\"\n",
    "\n",
    "Function:\n",
    "def get_coordinates_from_city(city_name):\n",
    "    \"\"\"\n",
    "    Fetches the latitude and longitude of a given city name using the Maps.co Geocoding API.\n",
    "\n",
    "    Args:\n",
    "    city_name (str): The name of the city.\n",
    "\n",
    "    Returns:\n",
    "    tuple: The latitude and longitude of the city.\n",
    "    \"\"\"\n",
    "\n",
    "User Query: {query}<human_end>\n",
    "\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "model_path=\"/home/miku/Model/nexusraven-v2-13b/nexusraven-v2-13b.Q4_K_M.gguf\"\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/home/miku/Model/qwq-32b-q4_k_m.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = prompt | llm\n",
    "question = \"What's the weather like in Seattle right now?\"\n",
    "llm_chain.invoke({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = prompt_template = \\\n",
    "'''\n",
    "Given 2 example:\n",
    "example 1:\n",
    "\"\"\"<example>\n",
    "#ID: 26967 \n",
    "\n",
    "#bin:\n",
    "ibcf 7.2.52\n",
    "\n",
    "#config:\n",
    "[PEER_valid_auto_ext]\n",
    "SIP_MANIPULATION[] = SET_ALLOW\n",
    "Authorization_profile = 0\n",
    "Max_in = 65550\n",
    "Max_out = 65550\n",
    "Hiding = NO_HIDING\n",
    "Media_Profile = 19\n",
    "Routing_profile = 0\n",
    "behavior = 7\n",
    "Trusted = TRUE\n",
    "Privacy = FALSE\n",
    "Fields_Filtering = NONE\n",
    "Domain[] = 192.168.33.33:5022\n",
    "\n",
    "[ MANIPULATION_RULE_DEL-ALLOW-INSTANCE1 ]\n",
    "Instance = \"^\"\n",
    "Action_Type = Del\n",
    "Element_Type = HEADER-VALUE\n",
    "Header_Name = Allow\n",
    "\n",
    "[ SIP_MANIPULATION_PROFILE_SET_ALLOW ]\n",
    "Manipulation[] = DEL-ALLOW-INSTANCE1\n",
    "Applicability = BOTH\n",
    "Method_Type[] = \"*\"\n",
    "Message_Type = BOTH\n",
    "Strategy = IN\n",
    "\n",
    "#Message:\n",
    "(11:15:05.88) W0 in <192.168.33.157:5681> <- 809 bytes from <192.168.33.163:5022>\n",
    "INVITE sip:0167000000@192.168.33.157:5681 SIP/2.0\n",
    "To: Called <sip:0167000000@192.168.33.157>\n",
    "From: Calling <sip:0166000000@192.168.33.163>;tag=1-1-1\n",
    "Via: SIP/2.0/UDP 192.168.33.163:5022;branch=z9hG4bK-13293-1-1\n",
    "Call-ID: 1-13293@192.168.33.163\n",
    "CSeq: 1 INVITE\n",
    "Contact: <sip:192.168.33.163:5022>\n",
    "P-Asserted-Identity: <sip:0166000000@192.168.33.163:5022;user=phone>\n",
    "Route: <sip:192.168.33.157:5681;lr>\n",
    "Max-Forwards: 70\n",
    "Allow: REFER,INVITE,INFO,ACK,SUBSCRIBE,UPDATE,REGISTER,BYE,CANCEL\n",
    "Content-Type: application/sdp\n",
    "Content-Length:   261\n",
    "\n",
    "v=0\n",
    "o=cp10 116221159107 116221159108 IN IP4 192.168.33.163\n",
    "s=SIP Call\n",
    "c=IN IP4 192.168.33.163\n",
    "t=0 0\n",
    "m=audio 6350 RTP/AVP 0 18 8\n",
    "b=AS:64\n",
    "a=rtpmap:0 PCMU/8000/1\n",
    "a=rtpmap:8 PCMA/8000/1\n",
    "a=rtpmap:18 G729/8000/1\n",
    "a=fmtp:18 annexb=no\n",
    "a=ptime:20\n",
    "a=sendrecv\n",
    "\n",
    "#ERROR:\n",
    "Allow: BYE,CANCEL,INFO,INVITE,REFER,REGISTER,SUBSCRIBE,UPDATE\n",
    "\n",
    "\n",
    "#Step reproduce:\n",
    "Step 1: Get the ibcf binary version 7.2.52\n",
    "Step 2: Get the basic call sipp scenarial\n",
    "Step 3: Modify header initial INVITE the same as provided INVITE message\n",
    "Step 4: Config HMR to delete first field in Allow header\n",
    "Step 5: Make the basic call between ibcf\n",
    "Step 6: Checking out going INVITE allow header is deleted not the first 'allow' value\n",
    "</example>\"\"\"\n",
    "example 2:\n",
    "\"\"\"<example>\n",
    "#ID:\n",
    "24628 \n",
    "\n",
    "#bin:\n",
    "ibcf 7.2.60\n",
    "\n",
    "#Config:\n",
    "[ MATCHING_RULE_HAS_TO_TAG ]\n",
    "Header_Name = TO\n",
    "Element_Type = HEADER-PARAM, tag\n",
    "Condition_Type = NONE\n",
    "Instance = \"*\"\n",
    "Min_Occurrence = 1\n",
    "\n",
    "#Message:\n",
    "(08:14:09.41) W0 in <10.208.189.17:5060> <- 3991 bytes from <10.208.189.14:5060>\n",
    "INVITE sip:+49D0781749490177@sipreg2.ewetel.de;user=phone SIP/2.0\n",
    "Call-ID: fdbb820566c142ebe980de659c7ec103@10.208.189.40\n",
    "Contact: <sip:+4944135012780@10.208.189.40:5060>\n",
    "Content-Type: application/sdp\n",
    "CSeq: 7 INVITE\n",
    "From: <sip:+4944135012780@sipreg2.voice.ewetel.de;user=phone>;tag=33038721_09ba9498_2bc2794a-788a-4b33-bf00-64d448102706\n",
    "Max-Forwards: 63\n",
    "Record-Route: <sip:10.208.189.14:5060;user=i0o0S000005b7;lr;Cpkt=CNMKV;C=on-cirpack>\n",
    "Route: <sip:10.208.189.17:5060;transport=udp;lr>,<sip:ims1p1.nica.ewetel.de;transport=udp;lr>\n",
    "Supported: 100rel,replaces\n",
    "To: <sip:+4944180002461@nplcrnat.osp.voice.ewetel.de>\n",
    "Via: SIP/2.0/UDP 10.208.189.14:5060;branch=z9hG4bK-CNMK-000010d4-198d2a4d,SIP/2.0/UDP 10.208.189.40:5060;received=10.208.189.40;rport=5060;branch=z9hG4bK2bc2794a-788a-4b33-bf00-64d448102706_09ba9498_7827518534571509\n",
    "Accept: application/sdp,multipart/mixed\n",
    "Accept-Encoding: identity\n",
    "Allow: INVITE,BYE,ACK,OPTIONS,CANCEL,INFO,PRACK,SUBSCRIBE,NOTIFY,REFER,UPDATE,MESSAGE,PUBLISH\n",
    "Allow-Events: telephone-event,refer\n",
    "Expires: 120\n",
    "User-Agent: AVM FRITZ!Box 7590 154.07.29 (Oct 26 2021)\n",
    "Session-ID: 05ef8fc121ed9814c6ad31e4cbe74262\n",
    "P-Asserted-Identity: <sip:+4944135012780@sipreg2.voice.ewetel.de;user=phone>\n",
    "History-info: <sip:+4944180007922@icscf-cl02.ims1.voice.ewetel.de?Privacy=history>;index=1\n",
    "History-info: <sip:+49800376462401@sipreg3.voice.ewetel.de;user=phone?Privacy=history&Reason=SIP%3Bcause%3D302%3Btext%3D%22Unconditional%22>;index=1.1\n",
    "History-info: <sip:+4944136111397@unknown.invalid;cause=380;user=phone;target=sip:+49800376462401%40unknown.invalid?Reason=SIP%3Bcause%3D486&Privacy=history>;index=1.1.1;mp=1.1\n",
    "History-info: <sip:+4944136111398@unknown.invalid;cause=486;user=phone;target=sip:+49800376462401%40unknown.invalid?Reason=SIP%3Bcause%3D487&Privacy=history>;index=1.1.2;mp=1.1\n",
    "History-info: <sip:+4944136111394@unknown.invalid;cause=487;user=phone;target=sip:+49800376462401%40unknown.invalid?Reason=SIP%3Bcause%3D500&Privacy=history>;index=1.1.3;mp=1.1\n",
    "History-info: <sip:+4944136111395@unknown.invalid;cause=404;user=phone;target=sip:+49800376462401%40unknown.invalid?Reason=SIP%3Bcause%3D500&Privacy=history>;index=1.1.4;mp=1.1\n",
    "History-info: <sip:+494413900195@unknown.invalid;cause=404;user=phone;target=sip:+49800376462401%40unknown.invalid?Reason=SIP%3Bcause%3D487&Privacy=history>;index=1.1.5;mp=1.1\n",
    "History-info: <sip:+4944139022692@unknown.invalid;cause=487;user=phone;target=sip:+49800376462401%40unknown.invalid?Reason=SIP%3Bcause%3D487&Privacy=history>;index=1.1.6;mp=1.1\n",
    "History-info: <sip:+494413900197@unknown.invalid;cause=487;user=phone;target=sip:+49800376462401%40unknown.invalid?Reason=SIP%3Bcause%3D487&Privacy=history>;index=1.1.7;mp=1.1\n",
    "History-info: <sip:+4944139022733@unknown.invalid;cause=487;user=phone;target=sip:+49800376462401%40unknown.invalid?Reason=SIP%3Bcause%3D487&Privacy=history>;index=1.1.8;mp=1.1\n",
    "History-info: <sip:+4944139063182@unknown.invalid;cause=487;user=phone;target=sip:+49800376462401%40unknown.invalid?Reason=SIP%3Bcause%3D487&Privacy=history>;index=1.1.9;mp=1.1\n",
    "History-info: <sip:+4944199869416@unknown.invalid;cause=487;user=phone;target=sip:+49800376462401%40unknown.invalid?Reason=SIP%3Bcause%3D487&Privacy=history>;index=1.1.10;mp=1.1\n",
    "History-info: <sip:+4944199869425@unknown.invalid;cause=487;user=phone;target=sip:+49800376462401%40unknown.invalid?Reason=SIP%3Bcause%3D487&Privacy=history>;index=1.1.11;mp=1.1\n",
    "History-info: <sip:+4944199869427@unknown.invalid;cause=487;user=phone;target=sip:+49800376462401%40unknown.invalid?Privacy=history>;index=1.1.12;mp=1.1\n",
    "Content-Length: 376\n",
    "\n",
    "#ERROR:\n",
    "W5 Header History-info : string too long, size=2204, max size=1024\n",
    "W0 Apply matching rule HIH_del_REID : false\n",
    "\n",
    "#Step reproduce:\n",
    "Step 1: Get the ibcf binary version 7.2.60\n",
    "Step 2: Get the basic call sipp scenarial\n",
    "Step 3: Modify header initial INVITE the same as provided INVITE message\n",
    "Step 4: Config HMR to delete HIH\n",
    "Step 5: Make the basic call between ibcf\n",
    "Step 6: Checking error log:\n",
    "W5 Header History-info : string too long, size=2204, max size=1024\n",
    "</example>\"\"\"\n",
    "\n",
    "Based on two examples, create a list of step to reproduce the new ERROR. Only print steps, no explaination. Finish with </end>:\n",
    "\"\"\"{query}\"\"\"\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23169 MiB free\n",
      "llama_model_loader: loaded meta data with 27 key-value pairs and 771 tensors from /home/miku/Model/qwq-32b-q4_k_m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Tmp_Quanted_Model AWInt4 Groupsize128\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 33B\n",
      "llama_model_loader: - kv   4:                          qwen2.block_count u32              = 64\n",
      "llama_model_loader: - kv   5:                       qwen2.context_length u32              = 131072\n",
      "llama_model_loader: - kv   6:                     qwen2.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   7:                  qwen2.feed_forward_length u32              = 27648\n",
      "llama_model_loader: - kv   8:                 qwen2.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   9:              qwen2.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  24:                                   split.no u16              = 0\n",
      "llama_model_loader: - kv  25:                        split.tensors.count i32              = 771\n",
      "llama_model_loader: - kv  26:                                split.count u16              = 0\n",
      "llama_model_loader: - type  f32:  321 tensors\n",
      "llama_model_loader: - type q4_K:  385 tensors\n",
      "llama_model_loader: - type q6_K:   65 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 18.48 GiB (4.85 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "load: special tokens cache size = 26\n",
      "load: token to piece cache size = 0.9311 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 5120\n",
      "print_info: n_layer          = 64\n",
      "print_info: n_head           = 40\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 5\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 27648\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 32B\n",
      "print_info: model params     = 32.76 B\n",
      "print_info: general.name     = Tmp_Quanted_Model AWInt4 Groupsize128\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 152064\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
      "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
      "print_info: EOG token        = 151663 '<|repo_name|>'\n",
      "print_info: EOG token        = 151664 '<|file_sep|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CUDA0\n",
      "load_tensors: layer   1 assigned to device CUDA0\n",
      "load_tensors: layer   2 assigned to device CUDA0\n",
      "load_tensors: layer   3 assigned to device CUDA0\n",
      "load_tensors: layer   4 assigned to device CUDA0\n",
      "load_tensors: layer   5 assigned to device CUDA0\n",
      "load_tensors: layer   6 assigned to device CUDA0\n",
      "load_tensors: layer   7 assigned to device CUDA0\n",
      "load_tensors: layer   8 assigned to device CUDA0\n",
      "load_tensors: layer   9 assigned to device CUDA0\n",
      "load_tensors: layer  10 assigned to device CUDA0\n",
      "load_tensors: layer  11 assigned to device CUDA0\n",
      "load_tensors: layer  12 assigned to device CUDA0\n",
      "load_tensors: layer  13 assigned to device CUDA0\n",
      "load_tensors: layer  14 assigned to device CUDA0\n",
      "load_tensors: layer  15 assigned to device CUDA0\n",
      "load_tensors: layer  16 assigned to device CUDA0\n",
      "load_tensors: layer  17 assigned to device CUDA0\n",
      "load_tensors: layer  18 assigned to device CUDA0\n",
      "load_tensors: layer  19 assigned to device CUDA0\n",
      "load_tensors: layer  20 assigned to device CUDA0\n",
      "load_tensors: layer  21 assigned to device CUDA0\n",
      "load_tensors: layer  22 assigned to device CUDA0\n",
      "load_tensors: layer  23 assigned to device CUDA0\n",
      "load_tensors: layer  24 assigned to device CUDA0\n",
      "load_tensors: layer  25 assigned to device CUDA0\n",
      "load_tensors: layer  26 assigned to device CUDA0\n",
      "load_tensors: layer  27 assigned to device CUDA0\n",
      "load_tensors: layer  28 assigned to device CUDA0\n",
      "load_tensors: layer  29 assigned to device CUDA0\n",
      "load_tensors: layer  30 assigned to device CUDA0\n",
      "load_tensors: layer  31 assigned to device CUDA0\n",
      "load_tensors: layer  32 assigned to device CUDA0\n",
      "load_tensors: layer  33 assigned to device CUDA0\n",
      "load_tensors: layer  34 assigned to device CUDA0\n",
      "load_tensors: layer  35 assigned to device CUDA0\n",
      "load_tensors: layer  36 assigned to device CUDA0\n",
      "load_tensors: layer  37 assigned to device CUDA0\n",
      "load_tensors: layer  38 assigned to device CUDA0\n",
      "load_tensors: layer  39 assigned to device CUDA0\n",
      "load_tensors: layer  40 assigned to device CUDA0\n",
      "load_tensors: layer  41 assigned to device CUDA0\n",
      "load_tensors: layer  42 assigned to device CUDA0\n",
      "load_tensors: layer  43 assigned to device CUDA0\n",
      "load_tensors: layer  44 assigned to device CUDA0\n",
      "load_tensors: layer  45 assigned to device CUDA0\n",
      "load_tensors: layer  46 assigned to device CUDA0\n",
      "load_tensors: layer  47 assigned to device CUDA0\n",
      "load_tensors: layer  48 assigned to device CUDA0\n",
      "load_tensors: layer  49 assigned to device CUDA0\n",
      "load_tensors: layer  50 assigned to device CUDA0\n",
      "load_tensors: layer  51 assigned to device CUDA0\n",
      "load_tensors: layer  52 assigned to device CUDA0\n",
      "load_tensors: layer  53 assigned to device CUDA0\n",
      "load_tensors: layer  54 assigned to device CUDA0\n",
      "load_tensors: layer  55 assigned to device CUDA0\n",
      "load_tensors: layer  56 assigned to device CUDA0\n",
      "load_tensors: layer  57 assigned to device CUDA0\n",
      "load_tensors: layer  58 assigned to device CUDA0\n",
      "load_tensors: layer  59 assigned to device CUDA0\n",
      "load_tensors: layer  60 assigned to device CUDA0\n",
      "load_tensors: layer  61 assigned to device CUDA0\n",
      "load_tensors: layer  62 assigned to device CUDA0\n",
      "load_tensors: layer  63 assigned to device CUDA0\n",
      "load_tensors: layer  64 assigned to device CUDA0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors: offloading 64 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 65/65 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size = 18508.35 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   417.66 MiB\n",
      "................................................................................................\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 8192\n",
      "llama_init_from_model: n_ctx_per_seq = 8192\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 32: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 33: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 34: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 35: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 36: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 37: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 38: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 39: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 40: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 41: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 42: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 43: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 44: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 45: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 46: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 47: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 48: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 49: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 50: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 51: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 52: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 53: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 54: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 55: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 56: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 57: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 58: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 59: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 60: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 61: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 62: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 63: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB\n",
      "llama_init_from_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_init_from_model:  CUDA_Host  output buffer size =     0.58 MiB\n",
      "llama_init_from_model:      CUDA0 compute buffer size =   696.00 MiB\n",
      "llama_init_from_model:  CUDA_Host compute buffer size =    26.01 MiB\n",
      "llama_init_from_model: graph nodes  = 2246\n",
      "llama_init_from_model: graph splits = 2\n",
      "CUDA : ARCHS = 860 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'split.count': '0', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.bos_token_id': '151643', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.embedding_length': '5120', 'split.tensors.count': '771', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Tmp_Quanted_Model AWInt4 Groupsize128', 'split.no': '0', 'qwen2.block_count': '64', 'general.type': 'model', 'general.size_label': '33B', 'qwen2.context_length': '131072', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n  {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" and not message.tool_calls %}\\n        {%- set content = message.content.split(\\'</think>\\')[-1].lstrip(\\'\\\\n\\') %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {%- set content = message.content.split(\\'</think>\\')[-1].lstrip(\\'\\\\n\\') %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'qwen2.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '27648', 'qwen2.attention.layer_norm_rms_epsilon': '0.000010', 'qwen2.attention.head_count': '40', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- '' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "  {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" and not message.tool_calls %}\n",
      "        {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "model_path=\"/home/miku/Model/nexusraven-v2-13b/nexusraven-v2-13b.Q4_K_M.gguf\"\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/home/miku/Model/qwq-32b-q4_k_m.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    temperature=0,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    "    n_ctx=8192,  # Set the context window size\n",
    "    max_tokens=32000,\n",
    "    stop=[\"</end>\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#Step reproduce"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 3256 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n",
      "Step 1: Get the ibcf binary version 7.2.65\n",
      "Step 2: Get the basic call sipp scenarial\n",
      "Step 3: Modify header initial INVITE the same as provided INVITE message\n",
      "Step 4: Config HMR to enforce TO tag presence\n",
      "Step 5: Make the basic call between ibcf\n",
      "Step 6: Checking error log:\n",
      "\"W0 Apply matching rule HAS_TO_TAG : false\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    3454.82 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3668.87 ms /   101 runs   (   36.33 ms per token,    27.53 tokens per second)\n",
      "llama_perf_context_print:       total time =    3888.72 ms /   102 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n#Step reproduce:\\nStep 1: Get the ibcf binary version 7.2.65\\nStep 2: Get the basic call sipp scenarial\\nStep 3: Modify header initial INVITE the same as provided INVITE message\\nStep 4: Config HMR to enforce TO tag presence\\nStep 5: Make the basic call between ibcf\\nStep 6: Checking error log:\\n\"W0 Apply matching rule HAS_TO_TAG : false\"\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = prompt | llm\n",
    "question = \"\"\"\n",
    "#ID:\n",
    "26716\n",
    "\n",
    "#bin:\n",
    "ibcf 7.2.65\n",
    "\n",
    "#Config:\n",
    "[ MATCHING_RULE_HAS_TO_TAG ]\n",
    "Header_Name = TO\n",
    "Element_Type = HEADER-PARAM, tag\n",
    "Condition_Type = NONE\n",
    "Instance = \"*\"\n",
    "Min_Occurrence = 1\n",
    "\n",
    "#ERROR:\n",
    "\"W0 Apply matching rule HAS_TO_TAG : false\"\n",
    "\"\"\"\n",
    "llm_chain.invoke({\"query\": question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
